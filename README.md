Loss Landscape Geometry and Optimisation Difficulty in Neural Networks

Overview
This repository accompanies a tutorial that investigates how the geometry of neural network loss landscapes influences optimisation difficulty beyond what standard performance metrics such as accuracy reveal. Although neural networks trained with different random initialisations often reach similar predictive performance, they may converge to geometrically distinct regions of the loss surface. This project demonstrates how such differences can be exposed through loss landscape analysis.

Using controlled experiments on the Fashion-MNIST dataset, the tutorial examines independent training runs of identical neural network architectures and analyses loss and accuracy behaviour along linear interpolation paths between trained solutions. The objective is to provide deeper intuition about optimisation dynamics, basin structure, and convergence behaviour in deep learning.

how to run code:
i used datset inbuilt just rub saif_ml_code.ipynb directly you will get results 

install numpy pandas matplotlip required packages

Project Contents
The repository contains a Jupyter notebook with all experiments, a PDF version of the tutorial, and a folder of figures generated by the experiments. These figures include training and validation learning curves, as well as loss and accuracy profiles along interpolated paths in parameter space. A license file is included to define permitted usage.

Dataset
All experiments use the Fashion-MNIST dataset, which contains 70,000 grayscale clothing images across ten classes. The standard training set is split into training and validation subsets, while the test set is reserved for final evaluation. Images are normalised and flattened into vector form to facilitate optimisation and loss landscape analysis. The dataset provides sufficient complexity to exhibit non-trivial optimisation geometry while remaining computationally efficient.

Methodology
A fully connected neural network is trained twice using identical architectures, hyperparameters, and optimisation settings. The only difference between training runs is the random weight initialisation. Despite converging to nearly identical test accuracy, the trained models follow distinct optimisation trajectories.

To analyse loss landscape geometry, linear interpolation is performed between the parameter vectors of the two trained models. Loss and accuracy are evaluated along this interpolation path to determine whether the solutions lie within the same basin or are separated by high-loss regions. Additional analysis of training and validation curves ensures that observed differences are not visible from standard learning dynamics alone.

Key Findings
The experiments show that models with near-identical predictive performance can occupy geometrically distant regions of parameter space. Linear interpolation reveals clear loss barriers between solutions, indicating disconnected basins in the loss landscape. These findings demonstrate that optimisation difficulty arises from high-dimensional landscape structure rather than poor local minima, and that accuracy alone is insufficient to characterise optimisation behaviour.

Reproducibility
All results and figures presented in the tutorial can be reproduced by running the provided notebook from start to finish using standard machine learning libraries. No external configuration or preprocessing beyond what is documented in the tutorial is required.

Accessibility Considerations
Figures are designed with high contrast and readable labels so they remain interpretable in grayscale. Legends and axes are clearly marked, and tables do not rely on colour for interpretation. The tutorial uses structured headings to support screen-reader navigation.

License
This project is released under the MIT License, allowing reuse, modification, and distribution with appropriate attribution.

References
The tutorial draws on established research in neural network optimisation and loss landscape analysis, including work by Goodfellow et al. (2015), Choromanska et al. (2015), Li et al. (2018), and the Deep Learning textbook by Goodfellow, Bengio, and Courville (2016).
